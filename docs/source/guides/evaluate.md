<!--
SPDX-FileCopyrightText: Copyright (c) 2024-2025, NVIDIA CORPORATION & AFFILIATES. All rights reserved.
SPDX-License-Identifier: Apache-2.0

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
-->

# Evaluating NVIDIA AgentIQ Workflows
AgentIQ provides a set of evaluators to run and evaluate the AgentIQ workflows. In addition to the built-in evaluators, AgentIQ provides a plugin system to add custom evaluators.

## Evaluating a Workflow
To evaluate a workflow, you can use the `aiq eval` command. The `aiq eval` command takes a workflow configuration file as input. It runs the workflow using the dataset specified in the configuration file. The workflow output is then evaluated using the evaluators specified in the configuration file.

To run and evaluate the simple example workflow, use the following command:
```bash
aiq eval --config_file=examples/simple/configs/eval_config.yml
```

## Understanding the Evaluation Configuration
The `eval` section in the configuration file specifies the dataset and the evaluators to use. The following is an example of an `eval` section in a configuration file:

`examples/simple/configs/eval_config.yml`:
```yaml
eval:
  general:
    output_dir: ./.tmp/aiq/examples/simple/
    dataset:
      _type: json
      file_path: examples/simple/data/langsmith.json
  evaluators:
    rag_accuracy:
      _type: ragas
      metric: AnswerAccuracy
      llm_name: nim_rag_eval_llm
```

The dataset section specifies the dataset to use for running the workflow. The dataset can be of type `json`, `jsonl`, `csv`, `xls`, or `parquet`. The dataset file path is specified using the `file_path` key.

## Understanding the Dataset Format
The dataset file provides a list of questions and expected answers. The following is an example of a dataset file:

`examples/simple/data/langsmith.json`:
```json
[
  {
    "id": "1",
    "question": "What is langsmith",
    "answer": "LangSmith is a platform for LLM application development, monitoring, and testing"
  },
  {
    "id": "2",
    "question": "How do I proptotype with langsmith",
    "answer": "To prototype with LangSmith, you can quickly experiment with prompts, model types, retrieval strategy, and other parameters"
  }
]
```

## Understanding the Evaluator Configuration
The evaluators section specifies the evaluators to use for evaluating the workflow output. The evaluator configuration includes the evaluator type, the metric to evaluate, and any additional parameters required by the evaluator.

### Display all evaluators
To display all existing evaluators, run the following command:
```bash
aiq info components -t evaluator
```

### Ragas Evaluator
[RAGAS](https://docs.ragas.io/) is an OSS evaluation framework that enables end-to-end
evaluation of RAG workflows. AgentIQ provides an interface to RAGAS to evaluate the performance
of RAG-like AgentIQ workflows.

`examples/simple/configs/eval_config.yml`:
```yaml
eval:
  evaluators:
    rag_accuracy:
      _type: ragas
      metric: AnswerAccuracy
      llm_name: nim_rag_eval_llm
    rag_groundedness:
      _type: ragas
      metric: ResponseGroundedness
      llm_name: nim_rag_eval_llm
    rag_relevance:
      _type: ragas
      metric: ContextRelevance
      llm_name: nim_rag_eval_llm
```

The following `ragas` metrics are recommended for RAG workflows:

`AnswerAccuracy`: Evaluates the accuracy of the answer generated by the workflow against the expected answer or ground truth.

`ContextRelevance`: Evaluates the relevance of the context retrieved by the workflow against the question.

`ResponseGroundedness`: Evaluates the `groundedness` of the response generated by the workflow based on the context retrieved by the workflow.

These metrics use a judge LLM for evaluating the generated output and retrieved context. The judge LLM is configured in the `llms` section of the configuration file and is referenced by the `llm_name` key in the evaluator configuration.

`examples/simple/configs/eval_config.yml`:
```yaml
llms:
  nim_rag_eval_llm:
    _type: nim
    model_name: meta/llama-3.3-70b-instruct
    temperature: 0.0000001
    top_p: 0.0001
    max_tokens: 2
```

### Trajectory Evaluator
This evaluator uses the intermediate steps generated by the workflow to evaluate the workflow trajectory. The evaluator configuration includes the evaluator type and any additional parameters required by the evaluator.

`examples/simple/configs/eval_config.yml`:
```yaml
eval:
  evaluators:
    trajectory:
      _type: trajectory
      llm_name: nim_trajectory_eval_llm
```

A judge LLM is used to evaluate the trajectory based on the tools available to the workflow.

The judge LLM is configured in the `llms` section of the configuration file and is referenced by the `llm_name` key in the evaluator configuration.

## Workflow Output
The `aiq eval` command runs the workflow on all the entries in the `dataset`. The output of these runs is stored in a file named `workflow_output.json` under the `output_dir` specified in the configuration file.

`examples/simple/configs/eval_config.yml`:
```yaml
eval:
  general:
    output_dir: ./.tmp/aiq/examples/simple/
```

If additional output configuration is needed you can specify the `eval.general.output` section in the configuration file. If the `eval.general.output` section is specified, the `dir` configuration from that section overrides the `output_dir` specified in the `eval.general` section.
```yaml
eval:
  general:
    output:
      dir: ./.tmp/aiq/examples/simple/
```

Here is a sample workflow output generated by running an evaluation on the simple example workflow:

`./.tmp/aiq/examples/simple/workflow_output.json`:
```
  {
    "id": "1",
    "question": "What is langsmith",
    "answer": "LangSmith is a platform for LLM application development, monitoring, and testing",
    "generated_answer": "LangSmith is a platform for LLM (Large Language Model) application development, monitoring, and testing. It provides features such as automations, threads, annotating traces, adding runs to a dataset, prototyping, and debugging to support the development lifecycle of LLM applications.",
    "intermediate_steps": [
      {
        >>>>>>>>>>>>>>> SNIPPED >>>>>>>>>>>>>>>>>>>>>>
      }
    ],
    "expected_intermediate_steps": []
  },
```
The contents of the file have been `snipped` for brevity.

## Evaluator Output
Each evaluator provides an average score across all the entries in the `dataset`. The evaluator output also includes the score for each entry in the `dataset` along with the reasoning for the score. The score is a floating point number between 0 and 1, where 1 indicates a perfect match between the expected output and the generated output.

The output of each evaluator is stored in a separate file under the `output_dir` specified in the configuration file.

Here is a sample evaluator output generated by running evaluation on the simple example workflow:

`./.tmp/aiq/examples/simple/rag_accuracy_output.json`:
```
{
  "average_score": 0.6666666666666666,
  "eval_output_items": [
    {
      "id": 1,
      "score": 0.5,
      "reasoning": {
        "question": "What is langsmith",
        "answer": "LangSmith is a platform for LLM application development, monitoring, and testing",
        "generated_answer": "LangSmith is a platform for LLM application development, monitoring, and testing. It supports various workflows throughout the application development lifecycle, including automations, threads, annotating traces, adding runs to a dataset, prototyping, and debugging.",
        "retrieved_contexts": [
          >>>>>>> SNIPPED >>>>>>>>
        ]
      }
    },
    {
      "id": 2,
      "score": 0.75,
      "reasoning": {
        "question": "How do I proptotype with langsmith",
        "answer": "To prototype with LangSmith, you can quickly experiment with prompts, model types, retrieval strategy, and other parameters",
        "generated_answer": "LangSmith is a platform for LLM application development, monitoring, and testing. It supports prototyping, debugging, automations, threads, and capturing feedback. To prototype with LangSmith, users can quickly experiment with different prompts, model types, and retrieval strategies, and debug issues using tracing and application traces. LangSmith also provides features such as automations, threads, and feedback capture to help users develop and refine their LLM applications.",
        "retrieved_contexts": [
          >>>>>>> SNIPPED >>>>>>>>
        ]
      }
    }
  ]
}
```
The contents of the file have been `snipped` for brevity.


## Adding Custom Evaluators
You can add custom evaluators to evaluate the workflow output. To add a custom evaluator, you need to implement the evaluator and register it with the AgentIQ evaluator system. See the [Custom Evaluator](custom-evaluator.md) documentation for more information.

## Additional Evaluation Options
For details on other evaluators and evaluation options, refer to [AgentIQ Evaluation Concepts](../concepts/evaluate.md) for more information.

##  Profiling and Performance Monitoring of AgentIQ Workflows
You can profile workflows via the AgentIQ evaluation system. For more information, refer to the [Profiler](profiler.md) documentation.
